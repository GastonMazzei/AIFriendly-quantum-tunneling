{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tunnel_generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9a8f31a6f572>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcategorical_crossentropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtunnel_generator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tunnel_generator'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from scipy.stats import bernoulli\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.losses import categorical_crossentropy\n",
    "\n",
    "from tunnel_generator import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Delta \\Psi = (V_{(x)}+E)\\Psi$$\n",
    "\n",
    "![](notebook-imgs/image1.png)\n",
    "![](notebook-imgs/image2.png)\n",
    "![](notebook-imgs/image3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_samples(d, partitions=4, **kwargs):\n",
    "    \"\"\"\n",
    "    kwargs: verbose=True\n",
    "    \"\"\"\n",
    "\n",
    "    # Shuffle and select the number of parts it \n",
    "    # will be split into\n",
    "    #\n",
    "    d = d.sample(frac=1)\n",
    "    p = 1/partitions\n",
    "    \n",
    "    # Define the number of samples per partition:\n",
    "    # equi-sampling will force us to select the \n",
    "    # occurrences of the less abundant\n",
    "    #\n",
    "    s = int(d.shape[0]/partitions)\n",
    "    for a in [x*p for x in range(partitions)]:\n",
    "        inner_s = sum(d.iloc[:,-1].apply(lambda x: round(x,1)).between(a,a+p))\n",
    "        if inner_s < s: s = inner_s\n",
    "    s = int(s)\n",
    "    if kwargs.get('verbose',True):\n",
    "        print(f'\\nBalancing samples:\\n{s} samples per bin with {partitions} bins '\\\n",
    "              f'will transform the {d.shape[0]}-points dataset\\ninto a '\\\n",
    "              f'{partitions*s}-points dataset!')\n",
    "    \n",
    "    # retrieve 's' items per class\n",
    "    #\n",
    "    data = []\n",
    "    for a in [x*p for x in range(partitions)]:\n",
    "        data += [d[d.iloc[:,-1].apply(lambda x: round(x,1)).between(a,a+p)].sample(frac=1).iloc[:s,:]]\n",
    "    if kwargs.get('verbose',True):\n",
    "        print('\\nDataset balanced successfully!\\n')\n",
    "    return pd.concat(data,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vainilla_main(required_length = 15E3, vainilla = True):\n",
    "    df = pd.DataFrame()\n",
    "    count = 0\n",
    "    q = 0\n",
    "    while ((len(df)<required_length) and (count<10)):\n",
    "        print(f'\\nIteration {count}:\\nRequired Length: {required_length}\\n'\\\n",
    "             f'Current Length: {len(df)}\\n')\n",
    "        if vainilla: df = balanced_samples(generator(\n",
    "                                    50+q,\n",
    "                                    50+q,\n",
    "                                    50+q,\n",
    "                                    verbose=False),\n",
    "        else: df = generator(\n",
    "                            3+q/5,\n",
    "                            3+q,\n",
    "                            3+q,\n",
    "                            verbose=False),\n",
    "                                          5, verbose=False)\n",
    "                                          5, verbose=False)\n",
    "        count += 1\n",
    "        q += 5\n",
    "\n",
    "    if len(df)>=required_length: print('\\nSUCCESS! the required-length-condition WAS SATISFIED')\n",
    "    else: print('\\nFAILURE: the required-length-condition WAS NOT SATISFIED')\n",
    "    print(f'\\n\\nREQUIRED: {required_length}\\nACTUAL: {len(df)}')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = vainilla_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               L          V    E         proba\n",
      "47616   0.505145  32.862626  2.9  5.171113e-04\n",
      "143699  1.616245  48.991919  0.2  8.817656e-16\n",
      "192233  2.121291  32.862626  4.1  1.855367e-14\n",
      "249920  2.727345  11.692929  8.6  6.630583e-06\n",
      "240379  2.626336  17.237374  7.9  6.094185e-10\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "df.to_csv('databases/tunnel-effect-database-vainilla.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
